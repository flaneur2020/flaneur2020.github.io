
最近想学习一下工业级的推理引擎的设计，不过 vLLM 和 SGLang 似乎都已经发展的比较复杂了。前段时间看到一个[nano-vllm](https://github.com/GeeeekExplorer/nano-vllm)，它的代码比较精简，但是也有一套完整的 Page Attention 和 Scheduler 的实现，国庆假期学习了一下，很有意思，在这里记录一下自己的理解。

感兴趣的点主要在：

- 怎样处理多个 request 的？
- 怎样抢占的？
- 怎样管理 kv cache 的内存的？

下面跟着感兴趣的这个点，过一下 nano-vllm 的代码。

## 代码结构

nano-vllm 的结构非常干净。除了内置一个 Qwen3 的模型结构定义，主要逻辑都在 `engine/` 目录，核心文件就这几个：

- `llm_engine.py`：对外暴露 `generate` 接口；接收请求交给 scheduler 处理。
- `sequence.py`：把请求封装为 `Sequence`，跟踪每个请求的生命周期和内存映射关系；
- `scheduler.py`：管理请求队列，处理请求的调度和抢占；
- `block_manager.py`：管理 KV Cache 内存块的分配和释放，以及按 hash 进行复用；
- `model_runner.py`：执行模型前向推理，scheduler 每一轮调度，均对应一次执行；

## 基本概念

### Page Attention

vLLM 快速流行的关键之一就是 Page Attention。nano-vllm 的核心设计也以此为基底，可以把它理解成 vLLM 的一个教学版实现。

在推理中，对内存管理的需求很直接：

1. 显卡的 KV Cache 总空间是固定的，除了模型权重等固定占用外，剩余显存都应当留给 KV Cache。怎样分配和管理这部分内存？
2. 用户请求的长度长短不同，每个请求对应的 KV Cache 也长短差异极大，如何在同一个 batch 内支持这些不同长度的请求？

Page Attention 借鉴了操作系统的 Page Cache，把整个用于 KV Cache 的内存切成固定大小的 Block，并维护“序列位置 → Block”的映射。这样做有几个好处：

1. 全局管理 KV Cache 相关的内存，通过分配表来管理 KV Cache 内存 Block 的分配与释放；
2. 允许为不同请求灵活拼装不同数量的 Block；
3. 可以在前缀一致时复用 Block，减少重复计算和存储。

### Sequence 和 Block

在 nano-vllm 的 Page Attention 实现中，两个最重要的概念是 `Sequence` 和 `Block`。

nano-vllm 的每个用户请求都会被封装成一个 `Sequence` 对象。

`Sequence` 对象的字段如下：

```python
class Sequence:
    def __init__(self, token_ids: list[int], sampling_params = SamplingParams()):
        self.seq_id = next(Sequence.counter)
        self.status = SequenceStatus.WAITING  # RUNNING, FINISHED
        self.token_ids = copy(token_ids)
        self.last_token = token_ids[-1]
        self.num_tokens = len(self.token_ids)
        self.num_prompt_tokens = len(token_ids)
        self.num_cached_tokens = 0
        self.block_table = []
        self.temperature = sampling_params.temperature
        self.max_tokens = sampling_params.max_tokens
        self.ignore_eos = sampling_params.ignore_eos
```


在这些字段里，最核心的是 `block_table`：这是一个整数列表，按序列位置保存对应的 KV Cache 内存的 Block ID。另一个重要的字段是 `status`，用于跟踪请求生命周期中的状态流转。

其他字段则基本是一些周边的信息。

在 nano-vllm 中，一个 Block 的大小等于 256 个 embedding；例如一个请求累计 1000 个 token，就会占用 4 个 Block。

`Block` 的结构更简单：

```python
class Block:
    def __init__(self, block_id):
        self.block_id = block_id
        self.ref_count = 0
        self.hash = -1
        self.token_ids = []
```

每个 Block 有唯一 ID、引用计数、`hash` 和 `token_ids` 四个字段。其中：

- `block_id` 和 KV Cache 物理内存块对应，用于地址映射，Sequence 中的 `block_table` 中所指向的，就是这里的 Block ID。
- `ref_count` 用来追踪共享：多个序列复用同一前缀时，引用计数大于 1。
- `hash` 用于前缀复用（Prefix Cache）的快速命中，稍后还会再提。

### Prefill 和 Decode

推理系统通常分为 Prefill 和 Decode 两个阶段：

- **Prefill 阶段**：计算用户输入的 prompt，把 KV Cache 填满该有的前缀。Prefill 算力密度高、序列长度长，通常一次只处理 1 个请求，多请求就 FIFO 排队。
- **Decode 阶段**：在已有 KV 的基础上增量生成，每次只长出 1 个 token。单序列 Decode 太“瘦”，利用率低，因此需要把多个请求合成一个 batch 一起跑。

由于 Prefill 和 Decode 两个阶段的特点不同，Scheduler 需要做一些协调工作：

- 跟踪请求处理状态：尚未 Prefill 的先补 Prefill；
- 从已 Prefill 完成的请求里，组一个尽可能大的 Decode batch；

此外，Scheduler 还是内存分配/释放的发起者：Prefill 前要分配足够的 Block；Decode 时随着序列变长也要按需继续分配。内存吃紧时则触发驱逐（或暂停部分请求）以腾挪空间。

## 主要流程

### 请求调度

从调度视角看，可以把一次完整的处理拆成「调度回合」。每一回合，Scheduler 做决策、准备输入，然后由 `model_runner` 执行一次前向。

一个常见的调度循环大致如下（简化描述，贴近 nano-vllm 的组织方式）：

1. 接收新请求，封装为 `Sequence`，状态初始为 WAITING。
2. 对尚未 Prefill 的请求，尝试进行 Prefill：
   - 计算需要的 Block 数量；若可分配，则分配并执行 Prefill 前向；
   - Prefill 完成后，该序列进入可 Decode 的集合；
3. 若存在可 Decode 的序列，按容量与策略组一个 batch，执行一次 Decode：
   - 每个序列生成 1 个新 token，更新 `last_token`、`num_tokens`；
   - 命中 EOS 或达到 `max_tokens` 的，标记 FINISHED，并回收其全部 Block；
4. 若内存不足以支撑下一次 Prefill/Decode，需要触发驱逐（见下节）。

上面这套循环体现了一个核心思想：Prefill 偏串行、Decode 偏并行；两者穿插进行，但每一回合只有一次前向。这样实现简单，易于验证，也便于观察 batch 规模与吞吐的变化。

### Block 分配与复用

Block 的分配由 `block_manager` 负责，配合 `Sequence.block_table` 维护映射关系：

- Prefill 阶段：先按 prompt 长度计算所需 Block 数量，一次性分配；同时写入对应 Block 的 KV。
- Decode 阶段：随着 token 增长，当前 Block 写满后，再追加分配下一个 Block，并把新 token 的 KV 写入。
- 前缀复用：当两个请求的前缀相同（hash 命中）时，直接让它们的 `block_table` 指向相同的 Block，`ref_count` 递增；后续只需为分叉后的新 token 分配新的 Block 即可。

这套机制的好处非常直接：

- 省内存：长公共前缀只存一份。
- 省算力：公共前缀只算一遍，Decode 后续在分叉处继续就行。

### 抢占与驱逐（内存紧张时怎么让路）

当下一次 Prefill 或 Decode 需要的 Block 数量超过可用上限时，Scheduler 需要“让路”。常见的做法包括：

- 暂停部分 Decode 较慢或积累较多的序列，把它们的 Block 逐出（或先标记为不可用），释放空间；
- 优先保留短序列或临近完成的序列，以最大化单位时间的完成数；
- 结合最近活跃度与引用计数，尽量不破坏高复用的前缀（驱逐代价更高）。

nano-vllm 的实现思路也围绕这些朴素的准则：遇到内存压力，先挑容易让路且收益高的序列回收其 Block；待资源充足时，再把被暂停的请求重新纳入队列继续 Decode。这样既能保证系统不崩（不 OOM），也能维持比较稳定的吞吐。

### 一个小例子（更直观一点）

假设同一时刻有两条请求进入：

- A：prompt 2k token，目标生成 128 token；
- B：prompt 200 token，目标生成 64 token。

调度通常会先 Prefill A（因为长、算力密集），紧接着 Prefill B。随后进入 Decode 回合，把 A、B 合成同一个 batch，同时各生成 1 个 token。随着 Decode 进行，B 可能先达到 `max_tokens` 或先命中 EOS，被标记 FINISHED 并回收全部 Block，A 则继续前进。若期间新请求 C 到来，且内存仍够，调度会在下个回合把 C Prefill 掉；如果内存不够，就先驱逐一部分“性价比”低的序列腾位置。

这种“Prefill 串行、Decode 并行”的节奏，在工程实践里非常常见，说人话就是：先把菜切好（Prefill），再一锅下去炒（Decode）。

## 小结

整体来看，nano-vllm 把 Page Attention 的关键要素都勾勒清楚了：Block 化的 KV 管理、`Sequence` 与 `block_table` 的映射、基于 hash 的前缀复用，以及围绕 Prefill/Decode 的调度循环。实现不复杂，但把“资源怎么切、批次怎么凑、内存不够怎么办”这三件事讲得很到位。

对我个人来说，最有启发的点有两个：

- 把调度回合固定为“一回合一次前向”，让很多工程细节（如输入拼接、状态推进、错误处理）变得简单直接。
- 在分配与驱逐上优先保护高复用前缀，可以用很小的代价换来明显的吞吐收益。

后续我还想进一步看两个方向：

- 前缀 hash 的冲突处理与一致性校验策略在大规模场景下的选择；
- Decode 期间更细粒度的优先级与抢占策略（例如让快完的序列优先过线）。