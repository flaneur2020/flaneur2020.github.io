
最近想学习一下工业级的推理引擎的设计，不过 vLLM 和 SGLang 似乎都已经发展的比较复杂了。前段时间看到一个[nano-vllm](https://github.com/GeeeekExplorer/nano-vllm)，它的代码比较简短，但是也有一个完整的 page attention 和 scheduler 的实现，国庆假期学习了一下，很有意思，在这里记录一下。

感兴趣的点主要在：

- 怎样处理多个 request 的？
- 怎样抢占的？
- 怎样管理 kv cache 的内存的？

## 代码结构

nano-vllm 的代码结构非常简洁，除了一个内置的 qwen3 模型的结构定义，主要代码在 engine/ 目录中，只有这几个文件：

- `llm_engine.py`：提供 generate 接口，把请求交给 scheduler 处理；
- `sequence.py`：请求被包成 Sequence，管理每个请求的生命周期和内存映射关系；
- `scheduler.py`：管理请求队列，管理调度和处理抢占；
- `block_manager.py`：管理 KV Cache 内存块的分配和释放，以及按 hash 进行复用；
- `model_runner.py`：执行模型前向推理，scheduler 每一次调度，均对应一次执行；

## 基本概念

### Page Attention

vLLM 之所以能快速流行，关键在于其 Page Attention 技术。nano-vllm 的核心设计也基于 Page Attention，可以看作是 vLLM 的精简实现。

在推理过程中，对内存管理的需求很直接：

1. 显卡的 KV Cache 总空间是固定的，除了模型权重等固定占用外，剩余显存都应当留给 KV Cache。怎样分配和管理这部分内存？
2. 用户请求的长度各不相同，导致每个请求对应的 KV Cache 长度也不一致，如何在同一个 batch 内高效地支持这些不同长度的请求？

Page Attention 的思路也很直接，就是借鉴操作系统的 Page Cache，将所有 KV Cache 内存细分为 Block 并管理映射，这样可以：

1. 全局管理 KV Cache 相关的内存，通过分配表来管理 KV Cache Block 的分配与释放；
2. 允许为不同的请求关联不同的 Block，从而支持不同长度的请求，乃至允许复用前缀，减少重复计算；

### Sequence 和 Block

在 nano-vllm 的 Page Attention 实现中，主要是 `Sequence` 和 `Block` 两个概念。

在 nano-vllm 中，每个用户请求都会被封装成一个 `Sequence` 对象。Sequence 对象主要包含以下信息：

- 请求的当前状态（waiting 或 running）；
- 根据用户 prompt 执行 Prefill 和 Decode 阶段生成的 KV Cache 内容；
- KV Cache 的 block 映射关系表；
