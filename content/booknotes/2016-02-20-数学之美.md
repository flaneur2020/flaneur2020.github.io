---
layout: default
title: 数学之美
---

# 读书笔记: 数学之美

<https://book.douban.com/people/fleure/annotation/10750155/>
## 信息的度量和作用

<原文开始>2010年举行了世界杯足球赛，大家都很关心谁是冠军。假如我错过了看世界杯，赛后我问一个知道比赛结果的观众“哪只球队是冠军？”他不愿意直接告诉我，而让我猜，并且我每猜一次，他要收一元钱才肯告诉我是否猜对了，那么我需要付给他多少钱才能知道谁是冠军呢？我可以把球队编上号，从1到32，然后提问：“冠军的球队在1-16号中吗？”假如他告诉我猜对了，我会接着问：“冠军在1-8号中吗？”假如他告诉我猜错了，我自然知道冠军队在9-16号中。这样只需要五次，我就能知道哪只球队是冠军。所以，谁是世界冠军这条消息的信息量只值5块钱。</原文结束>

<原文开始>信息量的比特数和所有可能情况的对数函数 log 有关。</原文结束>

<原文开始>有些读者此时可能会发现实际上可能不需要猜五次就能猜出谁是冠军，因为像西班牙、巴西、德国、意大利这样的球队冠军的可能性比日本、南非、韩国等球队大得多。因此，第一次猜测时不需要把32支球队等分成两个组，而可以把少数几只最可能的球队分成一组，把其他队分成另一组。然后猜冠军球队是否在那几支热门球队中。重复这样的过程，根据夺冠概率对剩下的候选球队分组，直到找到冠军队。这样，也许三次或四次就猜出结果。因此，当每支球队夺冠的可能性不等时，“谁是冠军球队”的信息量比5比特少。香农指出，它的准确信息量应该是： H=-(p1*log(p1) + p2*log(p2) + ... + p32*log(p32))</原文结束>
## 高阶语言模型

<原文开始>自然语言中，上下文之间的相关性可能跨度非常大，甚至可以从一个段落跨到另一个段落。因此，即使模型的阶数再提高，对这种情况也无可奈何，这就是马尔可夫假设的局限性，这时就要采用其他一些长程的依赖性（long distance dependency）来解决这个问题了。</原文结束>