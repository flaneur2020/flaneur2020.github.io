
## Softmax tiling

假设有一个向量的长度为 $2B$，将这个向量分成两个 $B$ 的子向量，针对其中的一个子向量：

$$
\begin{aligned}
  m(x^{(1)}) &= max_i(x^{(1)}_i) \\
  f(x^{(1)}) &= [ e^{x^{(1)}_1-m(x)} ... e^{x^{(1)}_B-m(x)} ] \\
  l(x^{(1)}) &= \sum_i{ f(x^{(1)})_i } \\
  softmax(x^{(1)}) &= \frac{f(x^{(1)})}{l(x^{(1)})}
\end{aligned}
$$

使向量 $x=[x^{(1)} \space  x^{(2)}] \in R^{2B}$ 。

已知两个子向量的最大值，求新的最大值：

$$
  m(x) = m([x^{(1)} x^{(2)}]) = max(m(x^{(1)}), m(x^{(2)}))
$$

$f(x)$ 的处理上有一点差异，$f(x^{(1)})$ 中指数项减去的是 $x^{(1)}$ 中的最大值 $m(x^{(1)})$，而 $f(x)$ 中需要减去的最大值是 $m(x)$。

$$
\begin{aligned}
  f(x) & = [e^{x_i-m(x)}...e^{x_{2B}-m(x)}] \\
       & = [e^{x_i-m(x^{(1)})+m(x^{(1)})-m(x)}...e^{x_{2B}-m(x^{(2)})+m(x^{(2)})-m(x)}] \\
       & = [ 
         e^{m(x^{(1)})-m(x)} f(x^{(1)}) \,\,\,\,
         e^{m(x^{(2)})-m(x)} f(x^{(2)}) ] \\
\end{aligned}
$$
可见基于两个子向量的 $f(x^{(1)})$ 和 $f(x^{(2)})$，和 $m(x^{(1)})$ 和 $m(x^{(2)})$，可以求出完整向量的 $f(x)$。

$l(x)$ 的计算类似：

$$
l(x) = e^{m(x^{(1)})-m(x)} l(x^{(1)}) +
         e^{m(x^{(2)})-m(x)} l(x^{(2)})
$$

不过这里还不大理想，按说 $f(x^{(1)})$ 和 $f(x^{(2)})$ 都是向量，在找到全局最大 $m(x)$ 回来算 $f(x)$ 仍然得遍历一遍 $f(x^{(1)})$ 和 $f(x^{(2)})$。

## Online Softmax

## FlashAttention 算法

思路是得到全局的 $m(x)$ 和 $l(x)$ 之后，重新回来算。

## References
- https://zhuanlan.zhihu.com/p/621272925
- [[From Online Softmax to FlashAttention]]