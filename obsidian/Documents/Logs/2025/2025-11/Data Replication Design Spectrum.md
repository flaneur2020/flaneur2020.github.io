Consistent data replication 有两个流派：

1. 默默地承受（tolerate）失败的节点
2. 对发现的错误进行显式的重配置（reconfiguration）

作者在这里列了三个点：

1. quorum based 的 leaderless replication 机制的 failure masking 算法；
2. reconfiguration based replication 配合 failure detection 算法；
3. leaderful consensus 作为最知名的 replication 方法，组合了两者；

![Diagram](https://transactional.blog/images/blog/2024-data-replication-design-spectrum/diag-pikchr-md5-d221f755d6342262a4b992d08e2b11be.svg)

## Failure Masking: Quorums

尽管无主 Paxos 与 ABD 在一致性保证上存在差异，它们在资源效率上高度相似：所有副本均存储完整数据副本，读写请求广播至全部副本，仅需多数响应即可推进。**“多数派”是故障掩蔽型算法的核心特征**——它要求系统容忍少数节点失效。

默认情况下，5 副本系统只能利用 20% 的存储空间。有如下方案可以优化，但是工业实现中会应用的比较少：

1. Witness Replica：允许 2/5 的副本仅存储版本号而非完整数据，若最新值只存在于 witness 节点，则读操作失败；
2. EC：5 副本只需要 3 副本就可以恢复原始数据。

无主 Quorum 算法的最大优势：**无单点故障**，任意节点失效均被屏蔽。

为了保持线性一致性，必须执行 Read Repair，若副本数据不一致，需要回写最新值。

多数派 Quorum 通常采用 **Last Writer Wins** 策略。

若应用需高频并发更新同一数据项，**无主共识可能非最优选择**。

### 故障检测：重配置型复制

重配置型复制算法依赖一个**预设的存活副本集合**，只有当该集合中所有节点均在线时，系统才能继续服务。

一旦检测到副本故障，系统会**主动移除故障节点**，并加入一个新存活节点进行替换——所有副本要么正常工作，要么被彻底剔除。

这类算法具有以下共性：

- 所有写入均广播至全部副本；
- 每个副本始终保存一份**完整、一致的数据快照**；
- 读取可由任意单个副本响应；
- 仅需 **n = f + 1** 个副本即可容忍 **f** 次故障（因剩余副本可重建数据）；
- **但它们均非共识算法**：无法自主决定分片归属、主节点选举等元数据问题，必须依赖**外部共识服务**（控制平面）。

这体现了典型的 **控制面 / 数据面分离**：

- **控制面**：一个轻量共识服务（如 ZK、etcd）管理副本组成员与分片映射；
- **数据面**：水平扩展的复制组，仅负责数据同步，无决策权。

重配置型复制分为两类：

1. 广播型：PacificaA
2. 链式：CRAQ，写入沿链传递（头→中→尾），读取仅允许从尾节点发起（CRAQ 允许任意节点读，但需等待写入完成）

所有系统在**故障检测 + 重配置期间完全不可用**，导致客户端延迟尖峰。