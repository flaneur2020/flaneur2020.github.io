http://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf

- 构建大规模系统的技巧：拆分服务；遵循性能和可靠性的设计模式；joint work with many many people；
- Google Web Search 从 99 年到 2010 年，规模增长了 1000 多倍
- Web Search 系统既缓存索引也缓存内容，命中率在 30%~60% 左右，在 index 更新或者缓存失效时可能有性能降低
- 扩容 Index Server 到最后使得所有的索引都可以放在内存里了
- 扩容中遇到问题：检索可能落到上千台机器，而不是局部在少数机器上；有些检索会把服务器弄挂，如果这样的检索落到上千台服务器上，这一千台服务器可能同时挂掉；
- Canary Request(金丝雀请求)：请求先落到一台机器上，如果请求成功，再发到其它节点上；如果请求失败，尝试下一台机器；如果失败 K 次，拒绝请求；这样好处是如果挂，只挂少数机器而不是上千台全挂...
- 如果机器多起来，你希望能：数据持久存储；大规模的计算能可靠地完成；
- 最早索引只是在批处理里面跑的，没有 checkpointing 出故障很头疼；排序 1Tb 数据很困难，最后允许 "Mosted sorted"
- Map Reduce：可靠的分布式计算，lost 1600 of 1800 machines once, but finished fine；
- 性能问题：慢的节点显著增加了执行时间；处理：多开几个相同的任务，选最快的结果；
经验
- 将大项目拆分成多个服务；工程师更容易上手：依赖更少；更容易开发测试；容易做更多实验；更容易重写服务，而不影响客户端；开发周期解耦，多个小团队可以并行地独立工作，更容易在全球分布办公；比如 google.com 会踩到 200+ 服务。
- 设计高效的系统：给出具体的问题定义，怎样寻找最优解？最优解可以是最简单、最高效或者最容易扩展；估算系统设计的容量；
- 了解基本构件（比如 MapReduce、GFS、protobuf、BigTable 等）：不仅了解接口，也要了解实现；
- 设计、构建基础设施：从 General 角度定位问题；编写软件来辅助定位问题；但是对不同需求需要不同对待，不一定需要将解决方案放在一个实现里面；建基于实际的问题，而不是想像潜在的问题，以控制复杂性；最好先自己用自己的基础设施！（更快地收集反馈验证哪些是否可行）
- 为扩展性而设计：预先估算哪些容量容易发生变化；不要设计无穷的扩展性：5倍~50倍就足够了；>100 倍，就需要重新思考和设计了；
模式
- 单个Master，千数 Workers：Master 协调全局状态：负载均衡、任务分配、故障修复；但尽量少地让客户端与 Master 交互；为 Master 开热备；客户端与 Worker 之间总是聚簇地传递数据。
	- 单 Master 优势：容易分析系统的完整状态；
	- 警告：需要仔细地设计 Master，避免各种不稳定因素；扩展到几千 Worker 即可，不能几万；
- 树形地分发请求；
	- 问题：Master 的网卡和CPU容易超载；
	- 方案：在 Master 与 Worker 之间增加中间层，合并 fan-in 数据；
- 备份请求以减少延时：
	- 问题：当 1k 台机器时，性能的方差会变大；
	- 方案：向多个节点同时发送请求，取最快的结果；
	- 当延时与具体的请求无关时，可以有效降低延时；会提高一点整体负载；可以有效降低长尾延时；
- 每台机器安排更小的执行单元：
	- 问题：希望减少机器崩溃重启的恢复时间；更精细地负载均衡；
	- 一般每台机器下 10~100 个执行单元；
- 可伸缩系统（Elastic System）：
	- 问题：估算容量困难；估多了浪费资源；估少了扛不住；
	- 设计系统时应允许在负载较低时缩容；在负载升高时扩容；当负载不均衡时，更激进地均衡负载；
- 多种实现的结合：
	- 分别满足实时性、正确性，相互结合
Final Thought
- 今天的趋势：大规模的数据中心＋数据更大规模与多样化＋客户端设备更强的计算能力
- 可能的机会：全球范围的分布式系统＋针对大量数据的更优化的 CPU ＋更多的工具