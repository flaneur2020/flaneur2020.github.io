## The neural net perspective

åœ¨ç¥ç»ç½‘ç»œçš„è¯­è¨€ä¸­ï¼ŒVAE åŒ…å«ä¸€ä¸ª encoderã€ä¸€ä¸ª decoder å’Œä¸€ä¸ª loss functionã€‚

encoder æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒçš„è¾“å…¥æ˜¯ $x$ï¼Œè¾“å‡ºä¸€ä¸ª hidden representation $z$ ï¼Œå®ƒçš„å‚æ•°æ˜¯ $\theta$ã€‚

$x$ å¯ä»¥æ˜¯ä¸€ä¸ª 28x28 çš„æ‰‹å†™æ•°å­—å¯¹åº”çš„åƒç´ ï¼Œæœ‰ 784 ä¸ªç»´åº¦ã€‚$z$ å¯¹åº”ä¸€ä¸ª latent space ä¸­çš„å‘é‡ï¼Œç»´åº¦æ•°è¿œå°äº 784ã€‚

è¿™é€šå¸¸è¢«ç§°ä½œä¸€ä¸ª â€œbottleneckâ€ï¼Œencoder å¿…é¡»å­¦ä¼šä¸€ä¸ªå‹ç¼©çš„è¡¨ç¤ºæ–¹æ³•ã€‚

encoder å¯ä»¥è¡¨ç¤ºä¸º $q_{\theta}(z | x)$ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªåˆ†å¸ƒä¸­ï¼Œé‡‡æ ·å¾—åˆ° $z$ çš„å€¼ã€‚

decoder æ˜¯å¦ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®ƒçš„è¾“å…¥æ˜¯ $z$ï¼Œå®ƒèƒ½å¤Ÿè¾“å‡ºåŸå§‹æ•°æ®çš„æ¦‚ç‡å¯†åº¦ï¼Œå®ƒè¡¨ç¤ºä¸º $q_{\phi}(x|z)$ã€‚

decoder è¿˜åŸåŸå§‹å›¾åƒä¸­è‚¯å®šä¼šæœ‰ä¸€å®šæŸå¤±ï¼Œæ€æ ·è¡¡é‡è¿™éƒ¨åˆ†æŸå¤±ï¼Ÿå¯ä»¥é€šè¿‡ $log \space p_{\phi}(x|z)$ æ¥è¡¡é‡ã€‚

VAE çš„ loss function ç›¸å½“äºä¸€ä¸ª <mark>negative log-likelihood åŠ ä¸Šä¸€ä¸ª regularizer</mark>ã€‚

ï¼ˆé‡æ¸©ä¸‹ negative log-likelihood çš„æŸå¤±å‡½æ•°ï¼š$l(\theta) = - \sum_{i=1}^{n} \left( y_i \log \hat{y}_{\theta, i} + (1 - y_i) \log (1 - \hat{y}_{\theta, i}) \right)$ï¼Œæ‰‹å†™å­—æ¯çš„è¯æ¯ä¸ªåƒç´ çš„è®­ç»ƒé›†ä¸­å¥½åƒåªæœ‰ 0 å’Œ 1 ä¸¤ä¸ªå–å€¼)

æ¯ä¸ªæ•°æ®ç‚¹çš„ loss functionï¼š

$$
\begin{equation}
l_i(\theta, \phi) = -\mathbb{E}_{z \sim q_\theta(z|x_i)} \left[ \log p_\phi(x_i | z) \right] + \text{KL}(q_\theta(z | x_i) \parallel p(z))
\end{equation}
$$
å…¬å¼çš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯ reconstruction lossï¼Œä¹Ÿå°±æ˜¯ç¬¬ $i$ ä¸ªè®­ç»ƒæ•°æ®çš„ negative log-likelyhoodã€‚è¿™ä¸€éƒ¨åˆ†ä¼šé¼“åŠ± VAE èƒ½å¤Ÿé‡å»ºå›¾åƒã€‚

å…¬å¼çš„ç¬¬äºŒä¸ªéƒ¨åˆ†æ˜¯ regularizerã€‚è¿™æ˜¯ KL æ•£åº¦ï¼Œè¡¡é‡ $q_{\theta}(z|x)$ å’Œ $p(z)$ çš„å·®å¼‚ã€‚è¿™é‡Œè¡¡é‡çš„æ˜¯ï¼Œ $q$ è¿™ä¸ªåˆ†å¸ƒå’Œ $p$ åˆ†å¸ƒçš„ç›¸ä¼¼æ€§ã€‚

åœ¨ VAE ä¸­ï¼Œ$p(z)$ æ»¡è¶³æ­£æ€åˆ†å¸ƒï¼Œå¹³å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ã€‚

å¦‚æœ encoder è¾“å‡ºçš„ $z$ åœ¨æ€»ä½“ä¸Šä¸æ»¡è¶³è¿™æ ·çš„æ­£æ€åˆ†å¸ƒï¼Œå°±ä¼šå—åˆ° loss function æƒ©ç½šã€‚

> This regularizer term means â€˜keep the representationsÂ ğ‘§zÂ of each digit sufficiently diverseâ€™

> If we didnâ€™t include the regularizer, the encoder could learn to cheat and give each datapoint a representation in a different region of Euclidean space.

å¦‚æœä¸åŒ…å«è¿™ä¸ª regularizerï¼Œencoder å®¹æ˜“ cheatï¼Œå–å·§ç»™æ¯ä¸ª datapoint ä¸€ä¸ªå•ç‹¬çš„è¡¨ç¤ºä½ç½®ã€‚æ¯”å¦‚åŒæ ·ä¸€ä¸ª â€œ2â€ï¼Œä¸¤ä¸ªäººå†™çš„ 2 åœ¨ $z$ ç©ºé—´ä¸­ä¼šè·ç¦»å¾ˆè¿œã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªè¯­ä¹‰ç©ºé—´æ›´åŠ æœ‰æ„ä¹‰ï¼Œå› æ­¤å¸Œæœ›ä¸åŒäººå†™çš„ â€œ2â€ åœ¨ latent space ä¸­éƒ½å°½é‡é è¿‘ã€‚

## The probability model perspective

åœ¨æ¦‚ç‡æ¨¡å‹ä¸­ï¼ŒVAE åŒ…å«ä¸€ä¸ªç‰¹å®šçš„æ¦‚ç‡æ¨¡å‹ï¼Œé’ˆå¯¹æ•°æ® $x$ å’Œ latent variable $z$ï¼Œ$p(x, z) = p(x | z) p(z)$ã€‚

å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ $i$ï¼š

1. å‡º latent variableï¼š$z_i \sim p(z)$
2. æ¢å¤åŸå§‹æ•°æ®ï¼š$x_i \sim p(x|z)$

æ ¹æ®è´å¶æ–¯å…¬å¼ï¼š

$$
p(z|x) = \frac{p(x|z) p(z)}{p(x)}
$$

$p(x)$ è¢«ç§°ä½œå…ˆéªŒã€‚