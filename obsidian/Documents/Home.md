prev: [[2023 - Index]]
## Jan

- Readings
	- AI/LLM
		- [[LLM Decoding Attention-KV Cache Int8 Quantization]]
		- [[Towards 100x Speedup - Full Stack Transformer Inference Optimization]]
		- [[Understanding and Overcoming the Challenges of Efficient Transformer Quantization]]
		- [[The case for 4-bit precision - k-bit Inference Scaling Laws]]
		- [[Towards Efficient Generative Large Language Model Serving - A Survey from Algorithms to Systems]]
		- [[Understanding Activation-Aware Weight Quantization (AWQ) - Boosting Inference Serving Efficiency in LLMs]]
		- [[AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration]]
		- [[The Illustrated Stable Diffusion]]
		- [[LoRA From Scratch – Implement Low-Rank Adaptation for LLMs in PyTorch]]
		- [[Efficient LLM inference solution on Intel GPU]]
	- GPU
		- [[Nvidia CUDA Core-LLM Decoding Attention Inference Optimization]]
		- [[CUDA Optimization - Part 1]]
		- [[CUDA Optimization - Part 2]]

- Books
	- 大学之路
	- [[怎样使孩子自觉又主动]]
## Feb
- Night
- Writings
	- [[q8 dot product 优化记录]]
- Readings
	- [[The end of 0% interest rates - what the new normal means for software engineers]]
	- Datasys
		- [[Apache Arrow DataFusion - A Fast, Embeddable, Modular Analytic Query Engine]]
	- GEMM
		- [[gemm - a rabbit hole]]
		- [[Optimization of GEMV on Intel AVX Processor]]
		- [[Anatomy of High-Performance Many-Threaded Matrix Multiplication]]
		- [[Anatomy of High-Performance Matrix Multiplication]]
- Books
	- 小而美
	- [[金钱心理学]]
## Mar
- Night
	- [[使用 rust 开发平民推理框架]]
	- k-quants
- Readings
	- [[GGUF, the long way around]]
	- [[Flash-Decoding for long-context inference]]
	- [[Fast Multidimensional Matrix Multiplication on CPU from Scratch]]
	- [[Matrix Multiplication on CPU]]
	- [[ServiceRouter - Hyperscale and Minimal Cost Service Mesh at Meta]]
	- [[Language Models - Completion and Chat-Completion]]
	- [[LLaMA Now Goes Faster on CPUs]]
- Books
	- [[Just Keep Buying]]
	- [[这就是 ChatGPT]]
	- [[财务自由之路]]
## April
- Readings
	- [[Large Language Models on CPUs]]
	- [["Matmul", a microcosm of AI performance]]
	- [[Experiments with Bitnet 1.5 (~ngmi~)]]
	- [[Towards 1-bit Machine Learning Models]]
	- [[Qwen2 GPT2 Tokenizer]]
	- [[Tokens for LLMs - Byte Pair Encoding in Go]]
	- [[Design choices for Vision Language Models in 2024]]
	- [[Parallel Grouped Aggregation in DuckDB]]
	- [[SIMD-friendly algorithms for substring searching]]
- Books
	- 躺不平的千禧一代
- Writings
	- [[BPE Tokenizer]]
## May
- Night
	- vulkan backend for crabml
	- RISC-V optimization
	- SME
- Readings
	- [[Beyond CUDA - GPU Accelerated C++ for Machine Learning on Cross-Vendor Graphics Cards Made Simple with Kompute]]
	- [[Vulkan Memory Types on PC and How to Use Them]]
	- [[LLM Inference - Continuous Batching and PagedAttention]]
	- [[PyramidInfer - Pyramid KV Cache Compression for High-throughput LLM Inference]]
	- [[GPUs Go Brrr]]
- Books
	- 周期、估值与人性
	- 我们从未中产过
	- [[读史求实]]
	- 中间地带的革命
## Jun
- night
	- [[Notes on Variational Autoencoder]]
	- vulkan backend for crabml
- Books
	- iPhone 摄影
	- 中间地带的革命
- Readings
	- [[Intuitively Understanding Variational Autoencoders]]
	- [[Tutorial - What is a variational autoencoder?]]
	- [[The evidence lower bound (ELBO)]]