prev: [[2023 - Index]]
## Jan

- Readings
	- AI/LLM
		- [[LLM Decoding Attention-KV Cache Int8 Quantization]]
		- [[Towards 100x Speedup - Full Stack Transformer Inference Optimization]]
		- [[Understanding and Overcoming the Challenges of Efficient Transformer Quantization]]
		- [[The case for 4-bit precision - k-bit Inference Scaling Laws]]
		- [[Towards Efficient Generative Large Language Model Serving - A Survey from Algorithms to Systems]]
		- [[Understanding Activation-Aware Weight Quantization (AWQ) - Boosting Inference Serving Efficiency in LLMs]]
		- [[AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration]]
		- [[The Illustrated Stable Diffusion]]
		- [[LoRA From Scratch – Implement Low-Rank Adaptation for LLMs in PyTorch]]
		- [[Efficient LLM inference solution on Intel GPU]]
	- GPU
		- [[Nvidia CUDA Core-LLM Decoding Attention Inference Optimization]]
		- [[CUDA Optimization - Part 1]]
		- [[CUDA Optimization - Part 2]]

- Books
	- 大学之路
	- [[怎样使孩子自觉又主动]]
## Feb
- Night
- Readings
	- [[The end of 0% interest rates - what the new normal means for software engineers]]
	- Datasys
		- [[Apache Arrow DataFusion - A Fast, Embeddable, Modular Analytic Query Engine]]
	- GEMM
		- [[gemm - a rabbit hole]]
		- [[Optimization of GEMV on Intel AVX Processor]]
		- [[Anatomy of High-Performance Many-Threaded Matrix Multiplication]]
		- [[Anatomy of High-Performance Matrix Multiplication]]
- Books
	- 小而美
	- [[金钱心理学]]
