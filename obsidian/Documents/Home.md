## Jan

- Readings
	- [[The Illustrated Stable Diffusion]]
	- [[LLM Decoding Attention-KV Cache Int8 Quantization]]
	- [[Towards 100x Speedup - Full Stack Transformer Inference Optimization]]
	- [[Understanding and Overcoming the Challenges ofEfficient Transformer Quantization]]
	- [[The case for 4-bit precision - k-bit Inference Scaling Laws]]
	- [[Nvidia CUDA Core-LLM Decoding Attention Inference Optimization]]
	- [[Towards Efficient Generative Large Language Model Serving - A Survey from Algorithms to Systems]]
	- [[MotherDuck - DuckDB in the cloud and in the client]]
