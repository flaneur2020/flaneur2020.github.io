## TLDR

- 这个研究强烈推荐使用 ReLU 而非 GELU、SiLU，ReLU 有很好的 sparsity 性质，可以大大优化推理的性能
- 作者研究了使用 ReLU 的 OPT Model，显示所有 layer 的 sparsity 都超过 90%

## Notes

> employing the Rectified Linear Unit (ReLU) activation function in neural networks is recognized for inducing sparse activations and has been adopted in various prior works


> we employ the OPT model, utilizing ReLU, and measure the sparsity of activations in the Feed Forward Network (FFN) between the fully connected layers. As illustrated in Fig. 1a, all layers exhibit sparsity exceeding 90%. On average, across all layers, this activation sparsity results in substantial weight transfer (I/O) savings between the GPU and CPU, impacting 95% of the rows of the down projection layer’s weights (Fig. 1b). This reduction directly translates to computation savings, as for these rows, the result of the matrix multiplication operation will be zero. Furthermore, unlike unstructured sparsity (e.g., weight pruning), this type of sparsity is more hardware-friendly due to zeroing more extensive and structured chunks, such as rows or columns


sparsity 意味着，大部分都是零值，矩阵相乘时候就都是零。


> we re-evaluate using ReLU for LLMs. We are motivated by the pragmatic consideration that, in many real-world applications and computational platforms capable of supporting sparse vector-matrix multiplications, com- putational efficiency during inference outweighs the one-time computational cost incurred during training. We make the following contributions:

> - We demonstrate that when trained from scratch, there is no significant difference in terms of performance between different activation functions.

>- Considering that many modern LLMs (e.g.,Llama and Falcon) have been trained with non-ReLU activations, and it is not cost-effective to train them from scratch, we investigate fine-tuning these models with ReLU activations. We show that the models quickly regain their original performance across various reasoning and reading comprehension tasks (Sec. 4.1). Moreover, we show that by leveraging the activation sparsity of ReLU layers and inserting additional ReLU layers after normalization layers, we can further reduce inference FLOPS by up to threefold


大多数应用来讲，在推理阶段消耗的算力远高于一次性的训练成本。

- 作者发现在从零训练开始，使用不同的 activation function 并没有显著的差异。
- 对于 Llama、falcon 这些并未使用 ReLU 的模型来讲，从零重新训练不划算，但是作者发现，通过 ReLU finetune 这些模型，也可以得到很好的性能；

> - In addition to their computational benefits, we present two promising applications of activation sparsity that can inspire future work. Firstly, we demonstrate that LLMs with ReLU activations reuse a significant portion of already activated neurons during token generation, a phenomenon we term _aggregated sparsity_ (Sec. 5.1). This reusability leads to an inference speedup for speculative decoding (Sec. 5.2). Additionally, we show that studying the pre-activations of pretrained LLMs can guide the selection of unconventional activation functions (e.g., shifted ReLU), achieving up to 90% sparsity while maintaining performance similar to ReLU activation (Sec. 5.3).

作者发现使用 ReLU 的 LLM 可以重用相当一部分已经激活的神经元，称作 _aggregated sparsity_。

> we show that replacing the activation functions of pretrained LLMs with ReLU is possible, and the performance can be recovered very rapidly during finetuning. Moreover, we show that we can exploit the sparse ReLU activations, and by inserting additional ReLU layers after normalization layers, we can improve inference efficiency, as FLOPS indicates. 

把激活函数替换成 ReLU 是可行的，可以在 finetune 阶段迅速地恢复性能。

此外，也可以通过在 normalization 层插入更多的 ReLU，提高推理的性能。

![[Screenshot 2023-12-22 at 21.52.34.png]]

> there are other matrix-vector multiplications in the decoder layer of transformers besides the down projection. For instance, before the up projection and gate projections of FFN layer, and QKV projections in the attention layer (see Fig. 3). Together, the mentioned matrix- vector multiplications consume about 55% of the total computation.

> To this end, we utilize the fact that in modern transformer layers, the input to both the attention and FFN layers come from a normal- ization layer, e.g., LayerNorm [4] or RMSNorm [78]. **These layers can be viewed as a specific form of MLP**, where, instead of applying arbitrary learnable parameters, they learn to scale inputs. Therefore, we apply ReLU to obtain sparse activations after normalization layers which we call the second stage of relufication in Fig.

除了基础的 ReLU 替换，还可以再更多阶段插入 ReLU，包括 before the up projection and gate projections of FFN layer, and QKV projections。

LayerRorm、RMSNorm 可以看做是一种特殊的 MLP。

### 5.1 Aggregated Sparsity: reusing previously activated neurons

> A consequence of using only a small subset of neurons for each token is that if these neurons are shared to some degree, the model still does not use all of the neurons until many tokens are processed. We refer to this as aggregated sparsity, which we defined as the ratio of neurons that have not been used up to processing the first t token.

### 5.3 The shifted ReLU activation

> comparing Fig. 4d with Fig. 4b revealed that the relufied Llama has much less sparsity (65%) than the relufied Falcon model (95%).

llama 做 ReLU 后的 sparsity 只有 65，相比 Falcon 95% 差不少。

> In addition, we build on two of our previous findings. First, the preactivation distribution of the relufied Llama (Fig. 5c) includes a considerable mass after the cutoff value at zero. Second, the shape of the preactivation distribution does not change before and after the relufication process (Fig. 5c and Fig. 5d).

> Therefore, we may be able to shift the preactivation distribution to the left to put more volume before the cutoff at 0. To this end, for preactivation input x, rather than applying ReLU(x), we use ReLU(x − b) where b ∈ R is a constant scalar.

但是 Llama 的 ReLU 激活值的分布也有类似的规律，可以给它调一个参，变成 $ReLU(x - b)$。这样调过之后它的 sparsity 能够提高而且性能没有明显影响。