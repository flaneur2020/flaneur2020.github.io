https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c

æµ®ç‚¹æ•°ä¸­çš„æ ¼å¼ä¸­åŒ…æ‹¬ï¼š

- Signï¼šä½¿ç”¨ 0 æˆ–è€… 1 è¡¨ç¤ºæ­£æ•°ã€è´Ÿæ•°ï¼›
- Exponentï¼šç”¨äºè¡¨ç¤ºæŒ‡æ•°ï¼Œä¸€èˆ¬æ˜¯ 2 ä½œä¸ºåº•æ•°ï¼›
- Significand/Mantissaï¼šæµ®ç‚¹æ•°çš„ç²¾åº¦ä¸ Mantissa çš„é•¿åº¦é«˜åº¦ç›¸å…³ï¼›

![[Pasted image 20230820100543.png]]

æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„æµ®ç‚¹ç±»å‹ï¼š

| type  | total | sign bit | exponent | Significand | Notes  |
| ----  | ----- | -------- | -------- | ----------- | -----  |
| FP32  | 32    | 1        | 8        | 23          | æœ‰æ¯”è¾ƒé«˜çš„ç²¾åº¦ï¼Œä½†æ˜¯æœ‰æ¯”è¾ƒé«˜çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ |
| FP16  | 16    | 1        | 5        | 10          | å†…å­˜å ç”¨æ›´å°‘ï¼Œä½†æ˜¯ç‰ºç‰²äº†è¡¨è¾¾çš„èŒƒå›´å’Œç²¾åº¦ï¼Œå¯èƒ½æœ‰æ•°å€¼ä¸ç¨³å®šæ€§ï¼Œå½±å“æ€§èƒ½|
| BF16  | 16    | 1        | 8        | 7           | ç›¸æ¯” FP16 æœ‰æ›´å¤§çš„è¡¨ç¤ºèŒƒå›´ï¼Œå‡å°‘ underflow å’Œ overflow é£é™©ï¼Œè™½ç„¶ç‰ºç‰²äº†ç²¾åº¦ï¼Œä½†æ˜¯å¯¹æ·±åº¦å­¦ä¹ ä»»åŠ¡æ€§èƒ½çš„å½±å“è¾ƒå°|

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼ŒFP32 é€šç•…ç§°ä½œâ€full precisionâ€œï¼ˆ4 bytesï¼‰ï¼Œè€Œ BF16 å’Œ FP16 è¢«ç§°ä½œâ€åŠç²¾åº¦â€œï¼ˆ2 bytesï¼‰ã€‚

èƒ½ä¸èƒ½è¿›ä¸€æ­¥ç”¨ INT8 æ¥è¡¨ç¤ºï¼Ÿ

# ğŸ”° NaÃ¯ve 8-bit Quantization

é‡åŒ–æŠ€æœ¯çš„ç›®æ ‡ï¼šå°†ä¸€ä¸ª FP32 çš„å‘é‡ $X$ æ˜ å°„åˆ°ä¸€ä¸ª INT8 çš„å‘é‡ $X_{quant}$ã€‚

å…ˆä»‹ç» absmax å’Œ zero point ä¸¤ç§é‡åŒ–æŠ€æœ¯ã€‚
### absolute maximum (absmax) quantization

æ±‚å‡ºå‘é‡ä¸­çš„æœ€å¤§å€¼ï¼Œä½¿æ¯ä¸ªæ•°å€¼é™¤ä»¥æœ€å¤§å€¼åï¼Œä¹˜ä»¥ 127ï¼Œä½¿ä¹‹åˆ†å¸ƒåœ¨ `[-127, 127]` èŒƒå›´å†…ï¼š

![[Pasted image 20230820102618.png]]

 å‡å¦‚å‘é‡ä¸­æœ€å¤§å€¼ä¸º $3.2$ï¼Œå¯¹ $0.1$ è¿™ä¸ªæ•°ä¼šé‡åŒ–ä¸º $round(0.1 \times 127 / 3.2) = 4$ï¼Œåé‡åŒ–å›æ¥ï¼š$4 \times 3.2 / 127 = 0.1008$ï¼Œäº§ç”Ÿäº† $0.0008$ çš„è¯¯å·®ã€‚
 
### zero-point quantization

åœ¨ ReLU ç­‰å‡½æ•°ä¸­åªä¼šè¾“å‡ºæ­£æ•°å€¼ã€‚è¾“å…¥çš„æ•°å€¼ä¼šåœ¨å‘é‡çš„æœ€å¤§å€¼ä¸æœ€å°å€¼ä¹‹é—´ï¼Œåˆ† 255 ä¸ªå—ã€‚ç„¶åé€‰æ‹©ä¸­é—´çš„ç¬¬ 128 ä¸ªæ•°å€¼ä¸ºé›¶ç‚¹ï¼Œè½¬æ¢åˆ° `[-128, 127]` èŒƒå›´å†…ã€‚

![[Pasted image 20230820103145.png]]
![[Pasted image 20230820103151.png]]

### benchmarks

![[Pasted image 20230820104025.png]]

Both plots are quite similar, with a surprising spike around 0. This spike shows that our quantization is quite lossy since reversing the process doesnâ€™t output the original values. This is particularly true for the absmax model, which displays both a lower valley and a higher spike around 0.

å¯¹æ•´ä¸ªæ¨¡å‹åšé‡åŒ–ä¼šæ¯”è¾ƒæ˜¾è‘—åœ°é™ä½æ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨å®è·µä¸­ï¼Œå¾€å¾€é‡‡å– vector-wise quantization çš„åŠæ³•ï¼Œä¼šå¯¹ tensor ä¸­çš„ä¸€ç»„è¡Œæˆ–è€…åˆ—æ•°æ®åšå•ç‹¬çš„é‡åŒ–ã€‚

ä½†æ˜¯ï¼Œå³ä½¿ vector-wise çš„é‡åŒ–ä¹Ÿä¸èƒ½è§£å†³ outlier feature çš„é—®é¢˜ã€‚transformer æ¨¡å‹åªè¦è¶³å¤Ÿå¤§ï¼ˆ>6.7Bï¼‰ï¼Œæå¤§æˆ–è€…æå°çš„ outlier-feature å€¼ä¼šåœ¨æ¯ä¸€å±‚éƒ½å­˜åœ¨ã€‚ä¸€ä¸ªå•ç‹¬çš„ outlier å€¼ä¾¿å¯ä»¥é™ä½æ‰€æœ‰å…¶ä»–æ•°å€¼çš„ç²¾åº¦ã€‚ä½†æ˜¯ç§»é™¤è¿™äº› outlier feature æ•°å€¼ä¹Ÿä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„æ€§èƒ½ã€‚

# 8-bit Quantization with LLM.int8()

LLM.int8 æ˜¯ [Dettmers et al. (2022)](https://arxiv.org/abs/2208.07339) ä¸­å¼•è¿›çš„ï¼Œç”¨äºè§£å†³ outlier é—®é¢˜ã€‚

å®ƒåœ¨ä¸€ä¸ª vector-wise çš„ absmax é‡åŒ–æŠ€æœ¯ä¹‹ä¸Šï¼Œå¼•è¿›äº†ä¸€ä¸ªæ··åˆç²¾åº¦é‡åŒ–ã€‚è®© outlier featuresåœ¨ä¸€ä¸ª FP16 æ ¼å¼ä¸­æ¥ä¿æŒç²¾åº¦ï¼Œå…¶ä»–å€¼é€šè¿‡ INT8 æ¥å¤„ç†ã€‚

outlier å¤§çº¦åªå  0.1% çš„æ€»æ•°ï¼Œæ€»çš„æ¥è¯´å¯ä»¥å‡å°‘ 50% çš„å†…å­˜å ç”¨ã€‚

![[Pasted image 20230820110252.png]]
LLM.int8() ä¹‹åï¼ŒçŸ©é˜µä¹˜æ³•åˆ†ä¸‰æ­¥ï¼š

1. å¯¹äº input hidden stateï¼ŒæŒ‰åˆ—å–å‡ºè¶…è¿‡ threshold çš„ outlierï¼Œæ‹†æˆä¸¤ä¸ªçŸ©é˜µï¼›
2. é’ˆå¯¹ FP16 çš„ outlier çŸ©é˜µï¼Œå’Œé outlier çš„ INT8 çŸ©é˜µï¼Œåˆ†å¼€åˆ†åˆ«æ‰§è¡ŒçŸ©é˜µä¹˜æ³•ï¼›
3. å°†ä¸¤ä¸ªç»“æœçŸ©é˜µå†å›æ¥ï¼›

![[Pasted image 20230820110835.png]]