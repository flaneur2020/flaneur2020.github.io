最近在 crabml 一直没对 tokenizer 上心，不过最近想支持下 Qwen2 模型，发现它的 tokenizer 还不大一样，在这里整理一下。

tokenizer 中的分词规则也是自动学习出来的，不过本文暂时不关注学习的过程，只关注推理时怎样根据现有的词表做分词。

tokenizer 能够将一段文本拆分为 token，然后将 token 对应的 embedding 作为对 LLM 的输入。

每个 LLM 都会带一个 token embedding table，作为 token 到 embedding 的映射表。

token embedding table 设置多大合适？

最直白的想法是在 embedding table 中针对每个英文单词对应一个 embedding。

但是这样有问题是，英文单词不见的是固定的，reddit 上时不时会出来一个网络语言词语比如 YOLO、HODL 这种。

何况，除了英文，还要中文、俄语、德语、阿拉伯语等等。所有词汇的数量不能说是无限，至少也是非常非常庞大的，比如中文的常见词汇有 20w 个。

无限庞大的词汇表，不管从存储和训练层面上看，都不经济。

另一个极端是，反正文本流都是字节，我就用 byte 当作一个 token。

这里的问题就成了，每个 byte 也就 256 个候选，填不够一千多维的 embedding，我猜 LLM 也不见得不能学，但是这样 context window 就浪费在一个字节一个字节这个层面上了。

所以一个好的 tokenizer 策略应该满足：

1. token 的种类足够多样，允许在 embedding 空间中分布开
2. 能够适应未知的词语、typo 输入，能够识别网络语言的造词等
3. 平均来看的每个 token 也尽量长一点，每个 token 越长，同一个 context window 下表示的内容就越多

所以 tokenizer 一般会介于 byte 与 word tokenizer 之间找一个均衡。BPE (Byte Piece Encoding) 就是很常用的编码方法。

## 分词过程

举个例子，`"i don't eat beaf."` 经过分词的输出可以是：

```
[
  "▁i",
  "▁don",
  "'",
  "t",
  "▁eat",
  "▁be",
  "af",
  "."
]
```

可见这里的分词并不是和英文单词一一对应的，一个英文单词很有可能被拆分为多个 token。

其中的 unicode 字符 `▁` 好像可以无脑替换为空格。在分词前，需要稍微做一下预处理：将文本的空格都替换为 `▁`，并在最前面补一个 `▁` 作为 prefix。

分词的开始是将原始的字符串拆解为 bytes，256 个 bytes 也就是词表的原始成员，也都是合法的 token：

```
["▁", "i", "▁", "d", "o", "n", "'", "t", "▁", "e", "a", "t", "▁", "b", "e", "a", "f", "."]
```

然后，遍历每个相邻的 pair，尝试合并一波。

相邻的两个词不一定位于词表中，比如 `"'t"`，这时就不会合并。

凡是位于词表中的词，比如 `▁i` ，都有一个分数。分数表大约长这样：

```
{
  "▁i": 0.40,
  "af": 0.12,
  "at": 0.20,
  "eat": 0.3,
  "▁eat": 0.2
  "be": 0.11,
  "▁be": 0.43,
  ...
}
```

在一次遍历中，会选出来所有 pair 组成的新的 token 的最高的分数，进行一次合并。

比如第一次遍历中，发现 "▁i" 的分数最高，那么 tokens 的长度会缩短一点，变成：

```
["▁i", "▁", "d", "o", "n", "'", "t", "▁", "e", "a", "t", "▁", "b", "e", "a", "f", "."]
```

每次遍历，tokens 列表都短一点，比如下次合并了 "a" 和 "t"：

```
["▁i", "▁", "d", "o", "n", "'", "t", "▁", "e", "at", "▁", "b", "e", "a", "f", "."]
```

再下次合并了 "e" 和 "at"，

```
["▁i", "▁", "d", "o", "n", "'", "t", "▁", "eat", "▁", "b", "e", "a", "f", "."]
```

再下次合并了 "▁" 和 "eat"：

```
["▁i", "▁", "d", "o", "n", "'", "t", "▁eat", "▁", "b", "e", "a", "f", "."]
```

这样一次合并一对 token，直到没有 pair 在 token 表中存在为止。

这种先按字节拆分，后根据分数 merge 为更长的 piece 的过程，就是 BPE 分词的典型过程。

## UTF8 字符

除了上面的分词过程，还需要考虑一个是 UTF-8 字符的编码。

在 llama 中，特殊字符编码为 `"<0xA2>"` 这样的格式。

一个 UTF8 单词比如 "牛肉"，在编码时，如果单词表中有 “牛” 这个字但是没有 “肉” 这个字，就会编码为 `["牛", "<0xe8>", "<0x82>", "<0x89>"]` 这样的序列。

每个 `"<0xNN>"` 这样的编码对应一个 0~255 的 byte 值，然后每个 byte 值都会对应一个 Token ID。

可以知道，通过 BPE tokenizer 对中文进行分词时，会遍历每个 unicode 字符，如果这个字符在 vocab 表中，则输出对应的 Token ID 即可，如果不在表中，再展开成 UTF-8 序列的形式进行编码。
## GPT2 Tokenizer