- IO Aware，flash attention 可以更少的内存访问；
- 目标是避免从 HBM 中读写 attention matrix；
- 这需要做到：
	- computing the softmax reduction without access to the whole input
	- not storing the large intermediate attention matrix for the backward pass.
- 将输入拆分为 blocks，tiling
- We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass,
- FLOPs 虽然有增加，但是跑得更快，GPT-2 跑的快 7.6 倍，用的内存也更少：linear in sequence length
- IO 复杂度是 O(N^2 d^2 M^-1)，d 是 head 维度，M 是 SRAM 的大小，
- A100 的 40~80gb 的 HBM，1.5~2.0tb/s
- 相对 SRAM 有 108 个 192KB，每个 streaming multiprocessors 一个 SRAM，对应带宽有 19TB/s
-
- ## 2.1 Hardware Performance
	- GPU 每个操作有海量的线程，称作一个 kernel
	- 每个 kernel 会将 HBM 中的数据注册到 SRAM，计算后写回给 HBM
	- operation 可以区分为 memory bound 或者 compute bound，这通常基于 “arithmetic intensity” 进行衡量
		- Compute bound: 操作的耗时主要由计算操作决定，典型的操作是矩阵相乘，大 inner dimension、大量 channel 的卷积
		- Memory bound：由内存操作的耗时决定，比如 <mark>elementwise（activation、dropout）、reduction（sum、softmax、batch norm、layer norm）</mark>
	- Kernel fusion
		- 加速内存操作最常见的做法是 kernel fusion：如果有多个操作涉及到同一个 input，那么这个 input 可以从 HBM 读一次，然后执行连续的操作；
		- Compiler 可以自动将多个 elementwise 的操作进行 fuse
		- 然而，在 model training 场景下，中间结果仍需要写回到 HBM，用于 backward pass，从而减少了 naive 的 kernel fusion 的作用
-
- ## 2.2 Standard Attention Implementation
	- 设 N 为 sequence 长度，d 为 head dimension，希望计算 attention output $$ O $$：
		- $$ S = Q K^{T} \in \textbf{R}^{N \times N}, P = softmax(S) \in \textbf{R}^{N \times N},  O = PV \in \textbf{R} ^{ N \times N }  $$
	- 传统的 attention 算法需要将 S 和 P 物化到 HBM 中，需要 $$ O(N^{2}) $$ 的内存
	- 大部分操作都是内存 bound 的
	- 在一些针对 attention matrix 的 elementwise 的操作之后，情况会更糟糕，比如针对 S 跑一个 masking 或者针对 P 跑一个 dropout
-
- ## 3.1 An Efficient Attention Algorithm With Tiling and Recomputation
	-