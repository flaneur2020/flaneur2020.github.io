---
layout: post
title: "[WIP] Math on VAE"
---

上篇 blog 中过了一下 VAE 的 intuition，还是很直白的，不过看介绍 VAE 背后的数学的文章总是觉得有点缺少连接感，于是除了吃饭睡觉都看数学（误）推了两个星期算是把公式推通了。感觉整个流程对我这种数学基础不好的来讲还挺吃力，在这里记录一下。

## 前置准备

在推公式之前，先把用到的数学公式复习/预习一下，后面就直接套进去就好。

### 概率的期望

可以用抽奖的例子来快速回忆起来概率的“期望”的概念：假如参加抽奖活动有一等奖和二等奖两种奖项，一等奖的概率是 1%，奖金 10000，二等奖的概率是 10%，奖金 100，问我参加这个抽奖活动可以赢得的奖金的期望是多少？

$$
\mathbb{E}[x] = \sum_{i=1}^{n} x_i p_i(x_i)
$$

其中 $x$ 是随机变量，$x_i$ 对应不同奖项的奖金，$p_i(x_i)$ 是对应不同奖项的概率。在这个例子中，我们抽奖得到奖金的期望是 $10000 \times 0.01 + 100 \times 0.1 = 110$。

对于连续的随机变量，期望的计算就是：

$$
\mathbb{E}[x] = \int x p(x) dx
$$

期望也可以通过采样的方法来估计，比如我们有一个样本集合 $X = \{x_1, x_2, \ldots, x_n\}$，那么期望可以这样估算：

$$
\mathbb{E}[x] \approx \frac{1}{n} \sum_{i=1}^{n} x_i, \,\,\,\, x_i \sim p(x)
$$

在推导中，我们可以借助这个换算，把连续的期望转换为采样的离散计算：

$$
\begin{align*}
\mathbb{E}[x] &= \int x p(x) dx \\
&\approx \frac{1}{n} \sum_{i=1}^{n} x_i, \,\,\,\, x_i \sim p(x)
\end{align*}
$$

比较典型的采样计算就是，在机器学习的一轮训练中，我们手里有一批训练数据集，就可以通过这组数据集作为样本集合来估算出期望的值。

期望的计算也可以代入到函数中：

$$
\begin{align*}
\mathbb{E}[f(x)] &= \int f(x) p(x) dx \\
&\approx \frac{1}{n} \sum_{i=1}^{n} f(x_i), \,\,\,\, x_i \sim p(x)
\end{align*}
$$

这里代入的函数一般就是 $\ln$，在概率分布是正态分布时，代入正态分布的公式可以最后推导为最小二乘法，这就和我们常见的损失函数联系起来了，在后面的推导中我们会具体地看到这个过程。

### KL 散度

KL 散度能够用来衡量两个概率分布之间的差异，定义如下：

$$
\text{KL}(p||q) = \int p(x) \log \frac{p(x)}{q(x)} dx
$$

KL 散度有一个性质就是总是大于零，在计算 $\text{ELBO}$ 作为下界时有用到这个性质。

根据苏神的博客里的说法，KL 散度也并非衡量两个概率分布的唯一办法，但是 KL 散度可以写成期望的形式，这在 VAE 的推导里面会比较方便。

### 贝叶斯公式

贝叶斯公式能够告诉我们如何根据新的证据（比如训练数据中的猫狗图）来调整我们对事件（某图像是合法的猫狗图片）的信念。在 VAE 的推导中，我们会代入贝叶斯公式来计算条件概率：

$$
p(z|x) = \frac{p(x|z)p(z)}{p(x)}
$$

## 图片生成模型

按概率的视角来看图片生成模型的话，我们假设 $p(x)$ 的概率分布是我们希望生成的图片的分布。

怎样理解这个概率分布？我们可以把 $x$ 对应一组 $128 \times 128$ 个像素组成的向量，对于一张有效的图片（比如猫狗照片），$p(x)$ 会接近 1，对于一张无效的乱码图片，$p(x)$ 会接近于 0。

生成图片的过程，就等于在 $p(x)$ 分布中采样。

但是 $x$ 作为一个高维度的随机变量，大部分可能的值按说都是无效的。我们不知道 $p(x)$ 的具体分布形式，我们并不知道怎样从 $p(x)$ 中采样。

针对不知道怎么采样的问题，VAE 在设定中会多安排一个隐变量 $z$，在生成图片时，我们先对正态分布的 $p(z)$ 进行随机采样，然后再通过 $p(x|z)$ 采样生成图片 $x$，使得 $p(x)$ 尽可能的接近 1。这一来每次采样，总是能生成出有效的图片。$p(x|z)$ 也被称作解码器（Decoder）。

至于 $z$ 可以看做是对原始图片的编码，编码器（Encoder） 会对图片 $x$ 按正态分布的 $p(z|x)$ 来采样，得到一个具体的 $z$。

![](//assets/images/2024-07-14-vae-01.png)

在训练的过程中，我们希望通过训练数据集 $x$ 来使 $p(x)$ 的概率最大化，从而同时学习得到一个好的编码器和解码器。在做图片生成时，就只拿解码器来用。

## 从最大化 $p(x)$ 到 ELBO

由于直接计算 $p(x)$ 会非常困难，VAE 在这里引入了变分推断的思想：引入一个近似的后验分布 $q(z|x)$ 使它来近似 $p(z|x)$ 的分布。 

优化的目标是最大化 $p(x)$，也就是最大化 $\ln p(x)$：

$$
\begin{align}
\ln p(x) 
&= \ln p(x) \textcolor{blue}{\underset{=1}{\int q(z|x) \, dz}} \\
&= \int q(z|x)  \ln p(x) \, dz \\
&= \int q(z|x) \ln \textcolor{blue}{\frac{ p(x|z) p(z) }{ p(z|x)}} \, dz  \\
&= \int q(z|x) \ln \frac{p(x|z) p(z) \, \textcolor{blue}{q(z|x)}}{p(z|x) \textcolor{blue}{q(z|x)}} \, dz \\
&= \underbrace{{\int q(z|x) \ln \frac{q(z|x)}{p(z|x)} \, dz}}_{KL \, Divergence} + \int q(z|x) \ln \frac{p(x|z)p(z)}{q(z|x)} dz \\
&= \underbrace{D_{KL} ( q((z|x) || p(z|x))}_{always\, \ge 0} + \int q(z|x) \ln \frac{p(x|z)p(z)}{q(z|x)} dz \\
&\ge \underbrace{\int q(z|x) \ln \frac{p(x|z)p(z)}{q(z|x)} dz}_{ELBO}
\end{align}
$$


在 $(1)$ 和 $(2)$ 中，代入等于 1 的 $\int q(z|x) dz$。

在 $(3)$ 中，代入贝叶斯公式，将 $p(x)$ 变换为 $\frac{p(x|z) p(z)}{p(z|x)}$。

在 $(4)$ 中，乘入等于 1 的 $\frac{q_\theta(z|x)}{q_\theta(z|x)}$，到 $(5)$ 中可以凑出来 $q(z|x)$ 和 $p(z|x)$ 的 KL 散度。

利用 KL 散度总是大于零的性质，我们可以得到 $\ln p(x)$ 的下界，也就是 ELBO：

$$
ELBO = \int q(z|x) \ln \frac{p(x|z)p(z)}{q(z|x)} dz
$$

$p(x)$ 总是大于 ELBO，所以我们可以通过最大化 ELBO 来最大化 $p(x)$。

## 从 ELBO 到 Training Objectives
